{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKPYxsCuUBbm"
   },
   "source": [
    "# **Stanford 3D Indoor Scene Dataset (S3DIS)**\n",
    "\n",
    "This notebook explores the [S3DIS](https://svl.stanford.edu/assets/papers/3D_Semantic_Parsing.pdf) dataset using the S3DIS data folder (which is about 30GB). It will show how to preprocess the data and store it in a reduced format with and without partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAIXJdDII92E"
   },
   "source": [
    "Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "omMUEoaEHqS2"
   },
   "outputs": [],
   "source": [
    "# !pip install open3d==0.16.0 # must be at least 0.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pD3z90hjUAir",
    "outputId": "603362a3-d322-472c-f411-524c14d27606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopen3d\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mo3\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# from open3d import JVisualizer # For Colab Visualization\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopen3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweb_visualizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m draw \u001b[38;5;66;03m# for non Colab\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/open3d/__init__.py:111\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing external Open3D-ML in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPEN3D_ML_ROOT\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m    110\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPEN3D_ML_ROOT\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopen3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_jupyter_labextension_paths\u001b[39m():\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called by Jupyter Lab Server to detect if it is a valid labextension and\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    to install the widget.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m            directory during widget installation.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/open3d/ml/__init__.py:35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopen3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpu\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpybind\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m configs\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vis\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/open3d/ml/datasets.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopen3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/open3d/_ml3d/datasets/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"I/O, attributes, and processing for different datasets.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msemantickitti\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SemanticKITTI\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ms3dis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S3DIS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparislille3d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParisLille3D\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoronto3d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Toronto3D\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/open3d/_ml3d/datasets/s3dis.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KDTree\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataProcessing, get_min_bbox, BEVBox3D\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import open3d as o3\n",
    "# from open3d import JVisualizer # For Colab Visualization\n",
    "from open3d.web_visualizer import draw # for non Colab\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moxHa_UJI7go"
   },
   "source": [
    "Get Dataset path and save path for new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = r'/home/t/Desktop/work/datasets/S3DIS/Stanford3dDataset'\n",
    "SAVE_PATH = r'/home/t/Desktop/work/datasets/S3DIS/Stanford3dDataset_v1.2_Reduced_Aligned_Version'\n",
    "PARTITION_SAVE_PATH = r'/home/t/Desktop/work/datasets/S3DIS/Stanford3dDataset_v1.2_Reduced_Aligned_Version'\n",
    "\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.mkdir(SAVE_PATH)\n",
    "\n",
    "if not os.path.exists(PARTITION_SAVE_PATH):\n",
    "    os.mkdir(PARTITION_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers for categories and colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    'ceiling'  : 0, \n",
    "    'floor'    : 1, \n",
    "    'wall'     : 2, \n",
    "    'beam'     : 3, \n",
    "    'column'   : 4, \n",
    "    'window'   : 5,\n",
    "    'door'     : 6, \n",
    "    'table'    : 7, \n",
    "    'chair'    : 8, \n",
    "    'sofa'     : 9, \n",
    "    'bookcase' : 10, \n",
    "    'board'    : 11,\n",
    "    'stairs'   : 12,\n",
    "    'clutter'  : 13\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique color map generated via\n",
    "# https://mokole.com/palette.html\n",
    "COLOR_MAP = {\n",
    "    0  : (47, 79, 79),    # ceiling - darkslategray\n",
    "    1  : (139, 69, 19),   # floor - saddlebrown\n",
    "    2  : (34, 139, 34),   # wall - forestgreen\n",
    "    3  : (75, 0, 130),    # beam - indigo\n",
    "    4  : (255, 0, 0),     # column - red \n",
    "    5  : (255, 255, 0),   # window - yellow\n",
    "    6  : (0, 255, 0),     # door - lime\n",
    "    7  : (0, 255, 255),   # table - aqua\n",
    "    8  : (0, 0, 255),     # chair - blue\n",
    "    9  : (255, 0, 255),   # sofa - fuchsia\n",
    "    10 : (238, 232, 170), # bookcase - palegoldenrod\n",
    "    11 : (100, 149, 237), # board - cornflower\n",
    "    12 : (255, 105, 180), # stairs - hotpink\n",
    "    13 : (0, 0, 0)        # clutter - black\n",
    "}\n",
    "\n",
    "map_colors = lambda x : COLOR_MAP[x]\n",
    "v_map_colors = np.vectorize(map_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain a Dict with all spaces and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_nums = '1-6' # decide on the number of areas to obtain\n",
    "area_dict = {}\n",
    "\n",
    "# get areas based on split\n",
    "areas = glob(os.path.join(ROOT, f'Area_[{area_nums}]*'))\n",
    "\n",
    "for area in areas:\n",
    "    # get all subfolders in area (corresponds to disjoint spaces (or locations))\n",
    "    spaces = next(os.walk(area))[1]\n",
    "\n",
    "    # get dict to store spaces\n",
    "    space_dict = {}\n",
    "\n",
    "    # for each space\n",
    "    for space in spaces:\n",
    "        space = os.path.join(area, space)\n",
    "        annotations = os.path.join(space, 'Annotations')\n",
    "\n",
    "        # get individual segmentation filepaths\n",
    "        segments = glob(os.path.join(annotations, '*.txt'))\n",
    "        \n",
    "        # update space dict\n",
    "        space_dict.update({space.split('\\\\')[-1] : segments})\n",
    "\n",
    "    # update area dict\n",
    "    area_dict.update({area.split('\\\\')[-1] : space_dict})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to obtain space data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_space_data(space_segments, categories=CATEGORIES):\n",
    "    ''' Obtains space data in (x,y,z),cat format all types are float32 \n",
    "        Inputs: \n",
    "            space_segments - (list) filepaths to all annotaed space segments \n",
    "                            for the current space.\n",
    "                            e.g. area_dict['Area_1']['conferenceRoom_2']\n",
    "            categories - (dict) maps string category to numeric category\n",
    "        Outputs:\n",
    "            space_data - (array) \n",
    "        '''\n",
    "    # space data list (x,y,z, cat)\n",
    "    space_data = []\n",
    "    for seg_path in space_segments:\n",
    "\n",
    "        # get truth category and xyz points\n",
    "        cat = CATEGORIES[seg_path.split('\\\\')[-1].split('_')[0]]\n",
    "        xyz = pd.read_csv(seg_path, header=None, sep=' ', \n",
    "                          dtype=np.float32, usecols=[0,1,2]).to_numpy()\n",
    "\n",
    "        # add truth to xyz points and add to space list\n",
    "        space_data.append(np.hstack((xyz, \n",
    "                                     np.tile(cat, (len(xyz), 1)) \\\n",
    "                                     .astype(np.float32))))\n",
    "\n",
    "    # combine into single array and return\n",
    "    return np.vstack(space_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now obtain reduced data format with no paritions (unsliced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'/home/t/Desktop/work/datasets/S3DIS/Stanford3dDataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(save_dir)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m space \u001b[38;5;129;01min\u001b[39;00m area_dict[area]:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# obtain xyz points with truth labels\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     space_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mget_space_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marea_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43marea\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mspace\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# save as .hdf5 file in new directory\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, space \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m, in \u001b[0;36mget_space_data\u001b[0;34m(space_segments, categories)\u001b[0m\n\u001b[1;32m     12\u001b[0m space_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seg_path \u001b[38;5;129;01min\u001b[39;00m space_segments:\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# get truth category and xyz points\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     cat \u001b[38;5;241m=\u001b[39m \u001b[43mCATEGORIES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mseg_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     17\u001b[0m     xyz \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(seg_path, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     18\u001b[0m                       dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32, usecols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# add truth to xyz points and add to space list\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: '/home/t/Desktop/work/datasets/S3DIS/Stanford3dDataset'"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "for area in area_dict:\n",
    "    # create new directory\n",
    "    save_dir = os.path.join(SAVE_PATH, area)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    for space in area_dict[area]:\n",
    "        # obtain xyz points with truth labels\n",
    "        space_data = pd.DataFrame(get_space_data(area_dict[area][space]))\n",
    "        \n",
    "        # save as .hdf5 file in new directory\n",
    "        save_path = os.path.join(save_dir, space + '.hdf5')\n",
    "        space_data.to_hdf(save_path, key='space_data')\n",
    "\n",
    "\n",
    "toc = time.time()\n",
    "print(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space_data = pd.read_hdf(os.path.join(save_dir, space + '.hdf5'), key='space_data').to_numpy()\n",
    "space_data = get_space_data(area_dict['Area_3']['conferenceRoom_1'], categories=CATEGORIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display basic point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0881579ab4c14ac888ec757dad4098af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WebVisualizer(window_uid='window_0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(space_data[:,:3])\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(space_data[:, 3])).T/255)\n",
    "\n",
    "draw(pcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out how to partition each point cloud into 1x1meter or just smaller chunks as in PointNet\n",
    "\n",
    "Here are the basic steps\n",
    "\n",
    "1) Shift the point cloud such that the min value is 0 for all axes\n",
    "2) Find the max of the shifted x and y dimensions to determine how many partitions to create\n",
    "3) Get x and y bins using np.histogram()\n",
    "4) Use the bins to get (x,y) space partitions\n",
    "5) Use the partitions to slice the space in the (x,y) dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could try the Point Net Approach and shift the min of the data to the origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slice(points, xyz_s, xpart, ypart):\n",
    "    ''' Obtains Point Cloud Slices from the (x,y) partitions \n",
    "        By default this will obtain roughly 1x1 partitions\n",
    "        inputs:\n",
    "            points - (array) could be xyz, rgb or any input array\n",
    "            xyz_s - (Nx3 array) 0 min shifter point cloud array \n",
    "            xpart - xpartitions [[lower, upper]]\n",
    "            ypart - ypartitions [[lower, upper]]\n",
    "        '''\n",
    "    x_slice = (xyz_s[:, 0] >= xpart[0]) \\\n",
    "              & (xyz_s[:, 0] <= xpart[1])\n",
    "\n",
    "    y_slice = (xyz_s[:, 1] >= ypart[0]) \\\n",
    "              & (xyz_s[:, 1] <= ypart[1])\n",
    "    \n",
    "    return points[x_slice & y_slice, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partitions(xyz, xyz_s, c=1.):\n",
    "    ''' Obtains Point Cloud Space Partitions\n",
    "        Inputs:\n",
    "            xyz_s - (Nx3 array) 0 min shifted point cloud array \n",
    "            c - (float) factor for deciding how many partitions to create (larger --> less partitions)\n",
    "        Outputs: \n",
    "            partitions - (tuple) x and y parition arrays with \n",
    "                         format: [[lower, upper]]\n",
    "        '''\n",
    "    ## get number of x, y bins\n",
    "    range_ = np.abs(xyz.max(axis=0) - xyz.min(axis=0))\n",
    "    num_xbins, num_ybins, _ = np.uint8(np.round(range_ / c))\n",
    "\n",
    "    # uncomment this to generate ~1x1m partitions\n",
    "    # num_xbins, num_ybins, _ = np.uint8(np.ceil(np.max(xyz_s, 0)))\n",
    "\n",
    "    ## get x, y bins\n",
    "    _, xbins = np.histogram(xyz_s[:, 0], bins=num_xbins)\n",
    "    _, ybins = np.histogram(xyz_s[:, 1], bins=num_ybins)\n",
    "\n",
    "    ## get x y space paritions\n",
    "    x_parts = np.vstack((xbins[:-1], xbins[1:])).T\n",
    "    y_parts = np.vstack((ybins[:-1], ybins[1:])).T\n",
    "\n",
    "    return x_parts, y_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146.38069939613342\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "num_invalid_partitions = 0\n",
    "\n",
    "for area in area_dict:\n",
    "    # create new directory\n",
    "    save_dir = os.path.join(PARTITION_SAVE_PATH, area)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    for space in area_dict[area]:\n",
    "        # obtain xyz points with truth labels\n",
    "        space_data = get_space_data(area_dict[area][space])\n",
    "\n",
    "        # obtain x, y partitions\n",
    "        xyz = space_data[:, :3]\n",
    "\n",
    "        # get 0 min shifted points\n",
    "        xyz_s = xyz - xyz.min(axis=0)\n",
    "        x_parts, y_parts = get_partitions(xyz, xyz_s, c=1.5)\n",
    "\n",
    "        # counter for parition saving\n",
    "        i = 0\n",
    "        for x_part in x_parts:\n",
    "            for y_part in y_parts:\n",
    "                \n",
    "                space_slice = pd.DataFrame(get_slice(space_data, xyz_s, x_part, y_part))\n",
    "\n",
    "                # only save if partition has at least 100 points:\n",
    "                if len(space_slice) > 100:\n",
    "                    i += 1\n",
    "                    save_path = os.path.join(save_dir, space + f'_partition{i}_.hdf5')\n",
    "                    space_slice.to_hdf(save_path, key='space_slice')\n",
    "                else:\n",
    "                    num_invalid_partitions += 1\n",
    "\n",
    "\n",
    "toc = time.time()\n",
    "print(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.377,  38.873,   2.54 ,   3.   ],\n",
       "       [-10.387,  38.875,   2.544,   3.   ],\n",
       "       [-10.523,  38.782,   2.433,   3.   ],\n",
       "       ...,\n",
       "       [ -9.831,  39.247,   1.533,   2.   ],\n",
       "       [ -9.833,  39.247,   1.637,   2.   ],\n",
       "       [ -9.833,  39.247,   1.691,   2.   ]], dtype=float32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_data = pd.read_hdf(save_path, key='space_slice').to_numpy()\n",
    "space_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c5d47f07194a00a0aa173fb2046a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WebVisualizer(window_uid='window_35')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(space_data[:, :3])\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(space_data[:, 3])).T/255)\n",
    "\n",
    "draw(pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMu+KmrIdBiqaSv6b1/RbGv",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c3515861ec4313dacaa20b0eec5bf326e6557b6589b7b6a4fe3c8baa566747d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
