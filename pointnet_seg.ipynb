{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Point Net for Segmentation**\n",
    "\n",
    "This notebook will explore the Segmentation head of Point Net using the reduced and paritioned version of S3DIS dataset. This version is similar to the one used by torch geometry except that we will have to normalize and sample data on the fly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassMatthewsCorrCoef\n",
    "import open3d as o3\n",
    "# from open3d import JVisualizer # For Colab Visualization\n",
    "from open3d.web_visualizer import draw # for non Colab\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP for supressing pytorch user warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize variables for entire notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "ROOT = r'/home/t/Desktop/work/datasets/S3DIS/Stanford3dDataset_v1.2_Reduced_Partitioned_Aligned_Version'\n",
    "\n",
    "# feature selection hyperparameters\n",
    "NUM_TRAIN_POINTS = 4096 # train/valid points\n",
    "NUM_TEST_POINTS = 1000 # test points\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See: https://www.mathworks.com/help/vision/ug/point-cloud-classification-using-pointnet-deep-learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    'ceiling'  : 0, \n",
    "    'floor'    : 1, \n",
    "    'wall'     : 2, \n",
    "    'beam'     : 3, \n",
    "    'column'   : 4, \n",
    "    'window'   : 5,\n",
    "    'door'     : 6, \n",
    "    'table'    : 7, \n",
    "    'chair'    : 8, \n",
    "    'sofa'     : 9, \n",
    "    'bookcase' : 10, \n",
    "    'board'    : 11,\n",
    "    'stairs'   : 12,\n",
    "    'clutter'  : 13\n",
    "}\n",
    "\n",
    "# unique color map generated via\n",
    "# https://mokole.com/palette.html\n",
    "COLOR_MAP = {\n",
    "    0  : (47, 79, 79),    # ceiling - darkslategray\n",
    "    1  : (139, 69, 19),   # floor - saddlebrown\n",
    "    2  : (34, 139, 34),   # wall - forestgreen\n",
    "    3  : (75, 0, 130),    # beam - indigo\n",
    "    4  : (255, 0, 0),     # column - red \n",
    "    5  : (255, 255, 0),   # window - yellow\n",
    "    6  : (0, 255, 0),     # door - lime\n",
    "    7  : (0, 255, 255),   # table - aqua\n",
    "    8  : (0, 0, 255),     # chair - blue\n",
    "    9  : (255, 0, 255),   # sofa - fuchsia\n",
    "    10 : (238, 232, 170), # bookcase - palegoldenrod\n",
    "    11 : (100, 149, 237), # board - cornflower\n",
    "    12 : (255, 105, 180), # stairs - hotpink\n",
    "    13 : (0, 0, 0)        # clutter - black\n",
    "}\n",
    "\n",
    "v_map_colors = np.vectorize(lambda x : COLOR_MAP[x])\n",
    "\n",
    "NUM_CLASSES = len(CATEGORIES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from s3dis_dataset import S3DIS\n",
    "\n",
    "# get datasets\n",
    "s3dis_train = S3DIS(ROOT, area_nums='1-4', npoints=NUM_TRAIN_POINTS, r_prob=0.25)\n",
    "s3dis_valid = S3DIS(ROOT, area_nums='5', npoints=NUM_TRAIN_POINTS, r_prob=0.)\n",
    "s3dis_test = S3DIS(ROOT, area_nums='6', split='test', npoints=NUM_TEST_POINTS)\n",
    "\n",
    "# get dataloaders\n",
    "train_dataloader = DataLoader(s3dis_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(s3dis_valid, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(s3dis_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(test_dataloader):\n",
    "    print(f\"Batch {i}, Data Shape: {data[0].shape}\")\n",
    "    if i == 2: break  # Just as an example to inspect the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an example and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, targets = s3dis_train[1000]\n",
    "\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points)\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(targets)).T/255)\n",
    "\n",
    "# draw(pcd)\n",
    "o3.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate class distributions in the Training dataset\n",
    "\n",
    "NOTE: If you take a look at the S3DIS dataset class, you will notice that samples with less points than npoints are sampled with replacement. This means that some points will be oversampled. This will lead to a variable class balance eachtime the dataloader is called. However, you can run these cells many times and see that we will nearly always have the same ratios of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_targets = []\n",
    "for (_, targets) in train_dataloader:\n",
    "    total_train_targets += targets.reshape(-1).numpy().tolist()\n",
    "\n",
    "total_train_targets = np.array(total_train_targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Barchart of category counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_bins = np.bincount(total_train_targets)\n",
    "\n",
    "plt.bar(list(CATEGORIES.keys()), class_bins, \n",
    "             color=[np.array(val)/255. for val in list(COLOR_MAP.values())],\n",
    "             edgecolor='black')\n",
    "plt.xticks(list(CATEGORIES.keys()), list(CATEGORIES.keys()), size=12, rotation=90)\n",
    "plt.ylabel('Counts', size=12)\n",
    "plt.title('Frequency of Each Category (Training - Areas 1-4)', size=14, pad=20);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this dataset is heavily imbalanced, the more common items such as ceiling, floor, and wall greatly outnumber all of the other categories. These classes are actually background classes. If we were to train using a loss such as Cross Entropy, we would consider each class to have an equal importance. Meaning, missclassifyng window is treated the same as misclassifying wall. In reality, we would want our model to focus more on the uncommon categories and not care so much about the common ones. We have a priori knowledge that structures such as ceiling, wall, and floor are continuous and if we look at some training examples, we will see that they are like large sheets. If we were to only classify 60% of these, we would still be in pretty good shape. However, if we classify only 60% of the other less common classes, we may run into some serious generalization issues. One way to overcome the class imbalance is to use a weighted loss function. This will essentially tell the model (during training) to focus more on particular classes that it gets wrong (hard examples) and to focus less on the classes it tends to get right (easy examples)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss that we will use is based off of the [Focal Loss](https://arxiv.org/pdf/1708.02002.pdf), which is a modified version of the Cross Entropy (CE) loss that is useful for imbalanced classification, it focuses the training on a sparse set of hard examples. \n",
    "\n",
    "We will also add the [Dice Loss](https://arxiv.org/pdf/1707.03237v3.pdf) to improve the IOU performance. See also this [paper](https://www.sciencedirect.com/science/article/pii/S2590005619300049#bib72) for Dice Loss. See [wiki](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point Net Segmentation Loss:\n",
    "\n",
    "$$ FL(p_n) = -\\alpha_{y_n} (1 - p_n)^\\gamma log(p_n)  $$\n",
    "$$ DL(p_n) = 1 - 2\\frac{|y_n \\cap p_n|}{|y_n| + |p_n|} $$\n",
    "\n",
    "$$ \\mathcal{L(p_n)} = FL(p_n) + DL(p_n) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Segmentation Version of Point Net\n",
    "Make a test forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointnet import PointNetSegHead\n",
    "\n",
    "points, targets = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointnet import PointNetSegHead\n",
    "\n",
    "points, targets = next(iter(train_dataloader))\n",
    "\n",
    "seg_model = PointNetSegHead(num_points=NUM_TRAIN_POINTS, m=NUM_CLASSES)\n",
    "out, _, _ = seg_model(points.transpose(2, 1))\n",
    "print(f'Seg shape: {out.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $\\alpha$ as the normalized inverse class frequency.\n",
    "\n",
    "Instead compute alpha roughly based on class frequencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model from disk if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = 'seg_balanced/seg_model_15.pth'\n",
    "\n",
    "# seg_model = PointNetSegHead(num_points=NUM_TRAIN_POINTS, m=NUM_CLASSES).to(DEVICE)\n",
    "# seg_model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from point_net_loss import PointNetSegLoss\n",
    "\n",
    "EPOCHS = 2\n",
    "LR = 0.0001\n",
    "\n",
    "# use inverse class weighting\n",
    "# alpha = 1 / class_bins\n",
    "# alpha = (alpha/alpha.max())\n",
    "\n",
    "# manually set alpha weights\n",
    "alpha = np.ones(len(CATEGORIES))\n",
    "alpha[0:3] *= 0.25 # balance background classes\n",
    "alpha[-1] *= 0.75  # balance clutter class\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "optimizer = optim.Adam(seg_model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-6, max_lr=1e-3, \n",
    "                                              step_size_up=1000, cycle_momentum=False)\n",
    "criterion = PointNetSegLoss(alpha=alpha, gamma=gamma, dice=True).to(DEVICE)\n",
    "\n",
    "seg_model = seg_model.to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our training we will want to quantify how well our model is performing. Typically we look at loss and accuracy, but for this segmentation problem we will need a metric that accounts for incorrect classification as well as correct classification. The Matthews Correlation Coefficient does this and we will use it to quantify our performance. The MCC ranges from -1 to 1, to understand what the MCC is reporting:\n",
    "- 0 indicates a random guess\n",
    "- -1 is the worst possible performance\n",
    "- 1 is the best possible performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_metric = MulticlassMatthewsCorrCoef(num_classes=NUM_CLASSES).to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to track how well the models learns structure, so we will keep track of the IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(targets, predictions):\n",
    "\n",
    "    targets = targets.reshape(-1)\n",
    "    predictions = predictions.reshape(-1)\n",
    "\n",
    "    intersection = torch.sum(predictions == targets) # true positives\n",
    "    union = len(predictions) + len(targets) - intersection\n",
    "\n",
    "    return intersection / union "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store best validation iou\n",
    "best_iou = 0.6\n",
    "best_mcc = 0.6\n",
    "\n",
    "# lists to store metrics\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "train_mcc = []\n",
    "train_iou = []\n",
    "valid_loss = []\n",
    "valid_accuracy = []\n",
    "valid_mcc = []\n",
    "valid_iou = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stuff for training\n",
    "num_train_batch = int(np.ceil(len(s3dis_train)/BATCH_SIZE))\n",
    "num_valid_batch = int(np.ceil(len(s3dis_valid)/BATCH_SIZE))\n",
    "\n",
    "save_dir = '/home/t/Desktop/work/pointnet/models'  # Change this to your desired path\n",
    "base_name = 'best_seg_model'\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # place model in training mode\n",
    "    seg_model = seg_model.train()\n",
    "    _train_loss = []\n",
    "    _train_accuracy = []\n",
    "    _train_mcc = []\n",
    "    _train_iou = []\n",
    "    for i, (points, targets) in enumerate(train_dataloader, 0):\n",
    "\n",
    "        points = points.transpose(2, 1).to(DEVICE)\n",
    "        targets = targets.squeeze().to(DEVICE)\n",
    "        \n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get predicted class logits\n",
    "        preds, _, _ = seg_model(points)\n",
    "\n",
    "        # get class predictions\n",
    "        pred_choice = torch.softmax(preds, dim=2).argmax(dim=2)\n",
    "\n",
    "        # get loss and perform backprop\n",
    "        loss = criterion(preds, targets, pred_choice) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step() # update learning rate\n",
    "        \n",
    "        # get metrics\n",
    "        correct = pred_choice.eq(targets.data).cpu().sum()\n",
    "        accuracy = correct/float(BATCH_SIZE*NUM_TRAIN_POINTS)\n",
    "        mcc = mcc_metric(preds.transpose(2, 1), targets)\n",
    "        iou = compute_iou(targets, pred_choice)\n",
    "\n",
    "        # update epoch loss and accuracy\n",
    "        _train_loss.append(loss.item())\n",
    "        _train_accuracy.append(accuracy)\n",
    "        _train_mcc.append(mcc.item())\n",
    "        _train_iou.append(iou.item())\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'\\t [{epoch}: {i}/{num_train_batch}] ' \\\n",
    "                  + f'train loss: {loss.item():.4f} ' \\\n",
    "                  + f'accuracy: {accuracy:.4f} ' \\\n",
    "                  + f'mcc: {mcc:.4f} ' \\\n",
    "                  + f'iou: {iou:.4f}')\n",
    "        \n",
    "    train_loss.append(np.mean(_train_loss))\n",
    "    train_accuracy.append(np.mean(_train_accuracy))\n",
    "    train_mcc.append(np.mean(_train_mcc))\n",
    "    train_iou.append(np.mean(_train_iou))\n",
    "\n",
    "    print(f'Epoch: {epoch} - Train Loss: {train_loss[-1]:.4f} ' \\\n",
    "          + f'- Train Accuracy: {train_accuracy[-1]:.4f} ' \\\n",
    "          + f'- Train MCC: {train_mcc[-1]:.4f} ' \\\n",
    "          + f'- Train IOU: {train_iou[-1]:.4f}')\n",
    "\n",
    "    # pause to cool down\n",
    "    time.sleep(4)\n",
    "\n",
    "    # get test results after each epoch\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # place model in evaluation mode\n",
    "        seg_model = seg_model.eval()\n",
    "\n",
    "        _valid_loss = []\n",
    "        _valid_accuracy = []\n",
    "        _valid_mcc = []\n",
    "        _valid_iou = []\n",
    "        for i, (points, targets) in enumerate(valid_dataloader, 0):\n",
    "\n",
    "            points = points.transpose(2, 1).to(DEVICE)\n",
    "            targets = targets.squeeze().to(DEVICE)\n",
    "\n",
    "            preds, _, A = seg_model(points)\n",
    "            pred_choice = torch.softmax(preds, dim=2).argmax(dim=2)\n",
    "\n",
    "            loss = criterion(preds, targets, pred_choice) \n",
    "\n",
    "            # get metrics\n",
    "            correct = pred_choice.eq(targets.data).cpu().sum()\n",
    "            accuracy = correct/float(BATCH_SIZE*NUM_TRAIN_POINTS)\n",
    "            mcc = mcc_metric(preds.transpose(2, 1), targets)\n",
    "            iou = compute_iou(targets, pred_choice)\n",
    "\n",
    "            # update epoch loss and accuracy\n",
    "            _valid_loss.append(loss.item())\n",
    "            _valid_accuracy.append(accuracy)\n",
    "            _valid_mcc.append(mcc.item())\n",
    "            _valid_iou.append(iou.item())\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f'\\t [{epoch}: {i}/{num_valid_batch}] ' \\\n",
    "                  + f'valid loss: {loss.item():.4f} ' \\\n",
    "                  + f'accuracy: {accuracy:.4f} '\n",
    "                  + f'mcc: {mcc:.4f} ' \\\n",
    "                  + f'iou: {iou:.4f}')\n",
    "        \n",
    "        valid_loss.append(np.mean(_valid_loss))\n",
    "        valid_accuracy.append(np.mean(_valid_accuracy))\n",
    "        valid_mcc.append(np.mean(_valid_mcc))\n",
    "        valid_iou.append(np.mean(_valid_iou))\n",
    "        print(f'Epoch: {epoch} - Valid Loss: {valid_loss[-1]:.4f} ' \\\n",
    "              + f'- Valid Accuracy: {valid_accuracy[-1]:.4f} ' \\\n",
    "              + f'- Valid MCC: {valid_mcc[-1]:.4f} ' \\\n",
    "              + f'- Valid IOU: {valid_iou[-1]:.4f}')\n",
    "\n",
    "\n",
    "        # pause to cool down\n",
    "        time.sleep(4)\n",
    "\n",
    "    # save best models\n",
    "    # Save only the best model\n",
    "    if valid_iou[-1] > best_iou:\n",
    "        best_iou = valid_iou[-1]\n",
    "        model_filename = f'{base_name}_epoch_{epoch}.pth'\n",
    "        model_save_path = os.path.join(save_dir, model_filename)\n",
    "        torch.save(seg_model.state_dict(), model_save_path)\n",
    "        print(f'Model saved to: {model_save_path}')\n",
    "        \n",
    "        # Print the path where the model is saved\n",
    "        print(f'Model saved to: {model_save_path}')\n",
    "\n",
    "    # if valid_mcc[-1] >= best_mcc:\n",
    "    #     best_mcc = valid_mcc[-1]\n",
    "    #     torch.save(seg_model.state_dict(), f'trained_models/seg_focal_dice_mcc/seg_model_{epoch}.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot the training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(8, 5))\n",
    "ax[0].plot(np.arange(1, EPOCHS + 1), train_loss, label='train')\n",
    "ax[0].plot(np.arange(1, EPOCHS + 1), valid_loss, label='valid')\n",
    "ax[0].set_title('loss')\n",
    "\n",
    "ax[1].plot(np.arange(1, EPOCHS + 1), train_accuracy)\n",
    "ax[1].plot(np.arange(1, EPOCHS + 1), valid_accuracy)\n",
    "ax[1].set_title('accuracy')\n",
    "\n",
    "ax[2].plot(np.arange(1, EPOCHS + 1), train_mcc)\n",
    "ax[2].plot(np.arange(1, EPOCHS + 1), valid_mcc)\n",
    "ax[2].set_title('mcc')\n",
    "\n",
    "ax[3].plot(np.arange(1, EPOCHS + 1), train_iou)\n",
    "ax[3].plot(np.arange(1, EPOCHS + 1), valid_iou)\n",
    "ax[3].set_title('iou')\n",
    "\n",
    "fig.legend(loc='upper right')\n",
    "plt.subplots_adjust(wspace=0., hspace=0.85)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/home/t/Desktop/work/pointnet/models/best_seg_model_epoch_162.pth'\n",
    "\n",
    "model = PointNetSegHead(num_points=NUM_TEST_POINTS, m=NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Quick test to gather metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_batch = int(np.ceil(len(s3dis_test)/BATCH_SIZE))\n",
    "\n",
    "total_test_targets = []\n",
    "total_test_preds = [] \n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # place model in evaluation mode\n",
    "    model = model.eval()\n",
    "\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    test_mcc = []\n",
    "    test_iou = []\n",
    "    for i, (points, targets) in enumerate(test_dataloader, 0):\n",
    "\n",
    "        if points.size(0) == 0:\n",
    "            print(f\"Batch {i} is empty. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        points = points.transpose(2, 1).to(DEVICE)\n",
    "        targets = targets.squeeze().to(DEVICE)\n",
    "\n",
    "        try:\n",
    "            preds, _, A = model(points)\n",
    "            pred_choice = torch.softmax(preds, dim=2).argmax(dim=2)\n",
    "\n",
    "            loss = criterion(preds, targets, pred_choice)\n",
    "\n",
    "            # get metrics\n",
    "            correct = pred_choice.eq(targets.data).cpu().sum()\n",
    "            accuracy = correct/float(BATCH_SIZE*NUM_TEST_POINTS)\n",
    "            mcc = mcc_metric(preds.transpose(2, 1), targets)\n",
    "            iou = compute_iou(targets, pred_choice)\n",
    "\n",
    "            # update epoch loss and accuracy\n",
    "            test_loss.append(loss.item())\n",
    "            test_accuracy.append(accuracy)\n",
    "            test_mcc.append(mcc.item())\n",
    "            test_iou.append(iou.item())\n",
    "\n",
    "            # add to total targets/preds\n",
    "            total_test_targets += targets.reshape(-1).cpu().numpy().tolist()\n",
    "            total_test_preds += pred_choice.reshape(-1).cpu().numpy().tolist()\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                print(f'\\t [{i}/{num_test_batch}] ' \\\n",
    "                      + f'test loss: {loss.item():.4f} ' \\\n",
    "                      + f'accuracy: {accuracy:.4f} ' \\\n",
    "                      + f'mcc: {mcc:.4f} ' \\\n",
    "                      + f'iou: {iou:.4f}')\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i}: {e}\")\n",
    "\n",
    "print(f'Final Results - Loss: {np.mean(test_loss):.4f} - Accuracy: {np.mean(test_accuracy):.4f} - MCC: {np.mean(test_mcc):.4f} - IOU: {np.mean(test_iou):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display test results\n",
    "torch.cuda.empty_cache()\n",
    "print(f'Test Loss: {np.mean(test_loss):.4f} ' \\\n",
    "        + f'- Test Accuracy: {np.mean(test_accuracy):.4f} ' \\\n",
    "        + f'- Test MCC: {np.mean(test_mcc):.4f} ' \\\n",
    "        + f'- Test IOU: {np.mean(test_iou):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_targets = np.array(total_test_targets)\n",
    "total_test_preds = np.array(total_test_preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Confusion Matrix for Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_confusion = pd.DataFrame(confusion_matrix(total_test_targets, total_test_preds),\n",
    "                              columns=list(CATEGORIES.keys()),\n",
    "                              index=list(CATEGORIES.keys()))\n",
    "\n",
    "test_confusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View test results on full space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # release GPU memory\n",
    "points, targets = s3dis_test.get_random_partitioned_space()\n",
    "\n",
    "if points.nelement() == 0 or targets.nelement() == 0:\n",
    "    print(\"Error: Loaded tensors are empty. Check data loading mechanism.\")\n",
    "\n",
    "\n",
    "# place on device\n",
    "points = points.to(DEVICE)\n",
    "targets = targets.to(DEVICE)\n",
    "\n",
    "\n",
    "# Normalize each partitioned Point Cloud to (0, 1)\n",
    "norm_points = points.clone()\n",
    "\n",
    "# Perform min subtraction\n",
    "min_vals = norm_points.min(axis=1)[0]\n",
    "norm_points = norm_points - min_vals.unsqueeze(1)\n",
    "\n",
    "# Perform division by max\n",
    "max_vals = norm_points.max(axis=1)[0]\n",
    "norm_points /= max_vals.unsqueeze(1)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # prepare data\n",
    "    norm_points = norm_points.transpose(2, 1)\n",
    "    targets = targets.squeeze()\n",
    "\n",
    "    # run inference\n",
    "    preds, _, _ = model(norm_points)\n",
    "\n",
    "    # get metrics\n",
    "    pred_choice = torch.softmax(preds, dim=2).argmax(dim=2)\n",
    "\n",
    "    loss = criterion(preds, targets, pred_choice)\n",
    "    correct = pred_choice.eq(targets.data).cpu().sum()\n",
    "    accuracy = correct/float(points.shape[0]*NUM_TEST_POINTS)\n",
    "    mcc = mcc_metric(preds.transpose(2, 1), targets)\n",
    "    iou = compute_iou(targets, pred_choice)\n",
    "\n",
    "print(f'Loss: {loss:.4f} - Accuracy: {accuracy:.4f} - MCC: {mcc:.4f} - IOU: {iou:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the truth and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display true full point cloud\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points.permute(2, 0, 1).reshape(3, -1).to('cpu').T)\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(targets.reshape(-1).to('cpu'))).T/255)\n",
    "\n",
    "draw(pcd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: Save point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.io.write_point_cloud('full.pcd', pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display true partitioned point cloud\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points.to('cpu')[2, :, :])\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(targets.to('cpu')[2, :])).T/255)\n",
    "\n",
    "# draw(pcd)\n",
    "o3.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predicted full point cloud\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points.permute(2, 0, 1).reshape(3, -1).to('cpu').T)\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(pred_choice.reshape(-1).to('cpu'))).T/255)\n",
    "\n",
    "draw(pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.io.write_point_cloud('full_predicted.pcd', pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predicted partitioned point cloud\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points.to('cpu')[2, :, :])\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(pred_choice.to('cpu')[2, :])).T/255)\n",
    "\n",
    "# draw(pcd)\n",
    "o3.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try displaying both point clouds??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display true full point cloud\n",
    "pcd_1 = o3.geometry.PointCloud()\n",
    "pcd_1.points = o3.utility.Vector3dVector(points.permute(2, 0, 1).reshape(3, -1).to('cpu').T)\n",
    "pcd_1.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(targets.reshape(-1).to('cpu'))).T/255)\n",
    "\n",
    "# display predicted full point cloud\n",
    "pcd_2 = o3.geometry.PointCloud()\n",
    "pcd_2.points = o3.utility.Vector3dVector(points.permute(2, 0, 1).reshape(3, -1).to('cpu').T + torch.Tensor([5, 0, 0]))\n",
    "pcd_2.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(pred_choice.reshape(-1).to('cpu'))).T/255)\n",
    "\n",
    "draw([pcd_1] + [pcd_2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the critical indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # release GPU memory\n",
    "points, targets = s3dis_test.get_random_partitioned_space()\n",
    "\n",
    "# place on device\n",
    "points = points.to(DEVICE)\n",
    "targets = targets.to(DEVICE)\n",
    "\n",
    "# Normalize each partitioned Point Cloud to (0, 1)\n",
    "norm_points = points.clone()\n",
    "norm_points = norm_points - norm_points.min(axis=1)[0].unsqueeze(1)\n",
    "norm_points /= norm_points.max(axis=1)[0].unsqueeze(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # prepare data\n",
    "    norm_points = norm_points.transpose(2, 1)\n",
    "    targets = targets.squeeze()\n",
    "\n",
    "    # run inference to get critical indexes\n",
    "    _, crit_idxs, _ = model(norm_points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate data into single point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcds = []\n",
    "crit_pcds = []\n",
    "for i in range(points.shape[0]):\n",
    "\t\n",
    "\tpts = points[i, :]\n",
    "\tcdx = crit_idxs[i, :]\n",
    "\ttgt = targets[i, :]\n",
    "\n",
    "\tpts_np = pts.cpu().numpy() if pts.is_cuda else pts.numpy()\n",
    "\n",
    "\t# get full point clouds\n",
    "\tpcd = o3.geometry.PointCloud()\n",
    "\tpcd.points = o3.utility.Vector3dVector(pts_np)\n",
    "\t\n",
    "\t# Ensure target tensor is moved to CPU before conversion\n",
    "\ttgt_cpu = tgt.cpu().numpy() if tgt.is_cuda else tgt.numpy()\n",
    "\tpcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(tgt_cpu)).T/255)\n",
    "\n",
    "\t# get critical set point clouds\n",
    "\tcritical_points = pts[cdx, :]\n",
    "\tcritical_points_np = critical_points.cpu().numpy() if critical_points.is_cuda else critical_points.numpy()\n",
    "\t# Ensure critical target tensor is moved to CPU before conversion\n",
    "\tcrit_tgt_cpu = tgt[cdx].cpu().numpy() if tgt[cdx].is_cuda else tgt[cdx].numpy()\n",
    "\tcritical_point_colors = np.vstack(v_map_colors(crit_tgt_cpu)).T/255\n",
    "\n",
    "\tcrit_pcd = o3.geometry.PointCloud()\n",
    "\tcrit_pcd.points = o3.utility.Vector3dVector(critical_points_np)\n",
    "\tcrit_pcd.colors = o3.utility.Vector3dVector(critical_point_colors)\n",
    "\n",
    "\tpcds.append(pcd)\n",
    "\tcrit_pcds.append(crit_pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(pcds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "pcds_combined = pcds[0]\n",
    "for p in pcds[1:]:\n",
    "    pcds_combined += p\n",
    "\n",
    "o3.io.write_point_cloud('full_set.pcd', pcds_combined)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw critical indexes of a single partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = points[0, :].to('cpu')\n",
    "cdx = crit_idxs[0, :].to('cpu')\n",
    "tgt = targets[0, :].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critical_points = pts[cdx, :]\n",
    "critical_point_colors = np.vstack(v_map_colors(tgt[cdx])).T/255\n",
    "\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(critical_points)\n",
    "pcd.colors = o3.utility.Vector3dVector(critical_point_colors)\n",
    "\n",
    "# o3.visualization.draw_plotly([pcd])\n",
    "# draw(pcd, point_size=5) # does not work in Colab\n",
    "draw(pcd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw Critical Indexes of the entire Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(crit_pcds, point_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "pcds_combined = pcds[0]\n",
    "for p in pcds[1:]:\n",
    "    pcds_combined += p\n",
    "\n",
    "o3.io.write_point_cloud('critical_set.pcd', pcds_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c3515861ec4313dacaa20b0eec5bf326e6557b6589b7b6a4fe3c8baa566747d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
